
# ğŸ¤– Local LLM Android App â€” Qwen 2.5 QA

An Android application that brings a local Qwen-based language model (LLM) to your phone for fast, private, and offline question-answering. Powered by ONNX Runtime and a Hugging Face-style BPE tokenizer, this app streams answers in real time â€” no internet needed.

---

## âœ¨ Features

- ğŸ“± On-device, offline question answering with ONNX Runtime
- ğŸ”¤ Hugging Face-compatible BPE tokenizer (`tokenizer.json`)
- ğŸ§  Qwen-compatible prompt formatting with streaming token generation
- ğŸš€ Coroutine-based inference for responsive performance
- ğŸ” Runs fully offline â€” your data stays on your device

---

## ğŸ“¸ Inference Preview

<p align="center">
  <img src="data/local%20llm%20screenshot1.jpg" alt="Input Prompt" width="20%" style="margin: 1%"/>
  <img src="data/local%20llm%20screenshot3.jpg" alt="Model Output 1" width="20%" style="margin: 1%"/>
  <img src="data/local%20llm%20screenshot4.jpg" alt="Model Output 2" width="20%" style="margin: 1%"/>
  <img src="data/local%20llm%20screenshot4.jpg" alt="Model Output 3" width="20%" style="margin: 1%"/>
</p>

<p align="center">
  <em>Figure: App interface showing prompt input and generated answers using the local LLM.</em>
</p>

---

## ğŸ§  Model Info

This app uses the **Qwen2.5-0.5B-Instruct** model optimized for instruction-following and QA tasks.

### ğŸ” Option 1: Use Preconverted ONNX Model

- Download [ONNX model](https://huggingface.co/onnx-community/Qwen2.5-0.5B-Instruct/tree/main/onnx) and [tokenizer.json](https://huggingface.co/onnx-community/Qwen2.5-0.5B-Instruct/tree/main) from Hugging Face.

### âš™ï¸ Option 2: Convert Model Yourself

Install Optimum with ONNX export support:

```bash
pip install optimum[onnxruntime]
# or
python -m pip install git+https://github.com/huggingface/optimum.git
```

Export the model:

```bash
optimum-cli export onnx --model Qwen/Qwen2.5-0.5B-Instruct qwen2.5-0.5B-onnx/
```

- You can also convert any fine-tuned variant by specifying the model path.
- Learn more about [Optimum here](https://huggingface.co/docs/optimum/main/en/index).

---

## âš™ï¸ Requirements

- [Android Studio](https://developer.android.com/studio)
- [ONNX Runtime for Android](https://github.com/microsoft/onnxruntime-genai/releases) (already included in this repo)
- A physical Android device for deployment and testing

---

## ğŸ“² How to Build & Run

1. Open Android Studio and create a new project (Empty Activity).
2. Name your app `local_llm`.
3. Copy all the project files from this repo into the appropriate folders.
4. Place your `model.onnx` and `tokenizer.json` in:
   ```
   app/src/main/assets/
   ```
5. Connect your Android phone using wireless debugging or USB.
6. To install:
   - Press Run â–¶ï¸ in Android Studio, **or**
   - Go to **Build â†’ Generate Signed Bundle / APK** to export the `.apk` file.

---

## ğŸ“¦ Download Prebuilt APK

â¡ï¸ [Download Pocket LLM APK](https://github.com/dineshsoudagar/Local-LLM-On-Andriod-Qwen-QA/releases/download/v1.0.0/pocket_llm_qwen2.5_0.5B_v1.0.0.apk)

---

## ğŸ” Privacy First

This app performs all inference locally on your device. No data is sent to any server, ensuring full privacy and low latency.

---

## ğŸ“„ License

MIT License â€” use freely, modify locally, and deploy offline. Contributions welcome!
